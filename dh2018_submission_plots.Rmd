---
title: Does language change accelerate? Modeling stylistic drifts over time
author: Maciej Eder
date: October 25, 2017
output: word_document
bibliography: bibliography_all.bib
csl: digital_humanities_abstracts.csl
---




```{r setup, include=FALSE}
# set global chunk options

library(knitr)
#library(plotly)
#library(fmsb)

load("first_results_100mfw_RUN-2.Rdata")
load("distinctive_features_1000mfw.Rdata")

words = table(unlist(lapply(current.year.results, function(x) x[, 2])))
words = names(words)
words.sd = c()
for(word in words) {
    # extracting the values for a given word, as a function of time
    current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
    # assigning 0 to the non-existent values
    current.word[is.na(current.word)] = 0
    current.word = sd(current.word)
    words.sd = c(words.sd, current.word)
}

names(words.sd) = words
words = names(sort(words.sd, decreasing = TRUE))

# getting rid of punctuation
words = words[grep("\\w+", words)]

# getting rid of "<p>", "million"
words = words[-c(grep("<p>", words))]
words = words[-c(grep("million", words))]

my.blue = rgb(0.15, 0.45, 0.96)
my.green = rgb(0.15, 0.85, 0.27, 0.7)
my.red = rgb(0.92, 0.3, 0.3, 0.6)
my.grey = rgb(0,0,0,.5)
my.orange = rgb(1,0.5,0.2,0.6) 
my.teal = rgb(0, 0.5, 0.5, 0.7)  #### or: my.teal = rgb(0.24, 0.65, 0.75, 0.7)
my.violet = rgb(0.75, 0.25, 0.82, 0.7)

opts_chunk$set(cache=TRUE)
```


## Introduction

One of the most interesting aspects of language development – overlooked in a vast majority of the existing studies – is the question of the dynamics of linguistic changes. Presumably, one should expect epochs of substantial stylistic drift followed by periods of stagnation, rather than purely linear trends. 

* a timeline-centric visualization
* ngram viewer
* COHA
* [@eder_historical_2016]


## Supervised classification and the timeline

To assess this issue, we apply an iterative procedure of automatic text classification. First, we formulate a working hypothesis that a certain year – be it 1750 – marks a major linguistic break. We divide the text samples into the ante and post subsets, according to particular texts’ publication date. Next, we randomly pick a number of train and test samples representing the both classes (_ante_ and _post_), and we train a supervised classifier. We perform a standard classification, and record the cross-validated accuracy rates. Then we dismiss the original hypothesis, in order to test new ones: we iterate over the timeline, testing the years 1755, 1760, 1765, 1770, ... for their discriminating power. The assumption is simple here: any acceleration of linguistic change will be reflected by higher accuracy scores.



```{r NSC_accuracy, echo=FALSE, message=FALSE, fig.width=8, fig.height=6, dpi=150}
breakpoint.dates = as.numeric(names(current.aut.results))

results.mean = sapply(current.aut.results, function(x) apply(x, 2, mean))[1,]
results.sd = sapply(current.aut.results, function(x) apply(x, 2, sd))[1,]

results.mean.400 = sapply(current.aut.results, function(x) apply(x, 2, mean))[2,]
results.mean.700 = sapply(current.aut.results, function(x) apply(x, 2, mean))[3,]
results.mean.1000 = sapply(current.aut.results, function(x) apply(x, 2, mean))[4,]

#png(filename = "coha_sequentially.png", width = 8, height = 6, res = 300, units = "in")

plot(results.mean ~ breakpoint.dates, ylim=c(0,100), type="n", xlab = "year", ylab = "classification accuracy [%]")
# adding vertical lines reaching -sd and +sd
segments(breakpoint.dates, results.mean-results.sd, 
         breakpoint.dates, results.mean+results.sd, 
         col=rgb(0,0,1,0.6))
# a baseline of 50% accuracy
abline(h=50, lty=2, col="grey")
text(1970,50, labels = "baseline", pos = 3, cex = 0.6)
# a linear model showing the change
#abline(lm(results.mean ~ breakpoint.dates), col=rgb(1,0,0,0.6))
points(results.mean ~ breakpoint.dates, col=rgb(0,0,1,0.6), lwd=2)


arrows(1865,100,1865,82, length = 0.1, col = "grey")
text(1865,100, labels = "1865", pos = 2, srt = 90, cex = 0.6)
arrows(1893,100,1893,73, length = 0.1, col = "grey")
text(1893,100, labels = "1893", pos = 2, srt = 90, cex = 0.6)
arrows(1929,100,1929,80, length = 0.1, col = "grey")
text(1929,100, labels = "1929", pos = 2, srt = 90, cex = 0.6)
arrows(1972,100,1972,70, length = 0.1, col = "grey")
text(1972,100, labels = "1972", pos = 2, srt = 90, cex = 0.6)
```


_Figure 3: A sequence of Nearest Shrunken Classification tests on 333 English texts: cross-validated results for different vectors of most frequent POS-tag 2-grams_


In Fig. 3, the classification accuracy rates for the aforementioned corpus of 333 English texts were shown (POS-tag 2-grams, NSC classifier). As one can observe, the scores obtained for the period 1750–1850 are only slightly higher than the baseline, betraying no revolutionary changes in this period. Later, however, the stylistic drift accelerates, reaching 70% of correctly recognized test samples.


## Distinctive features


```{r coha_features, echo=FALSE, message=FALSE, fig.width=8, fig.height=6, dpi=150}
#png(filename = "coha_features.png", width = 8, height = 6, res = 300, units = "in")

threshold = 2

word = "you"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
plot(current.word ~ c(1835:1984), ylim=c(-.3,.3), type = "n", xlab = "year", ylab = "discrimination strength")

arrows(1871,0.3,1871,0.2, length = 0.1, col = "grey")
text(1871,0.3, labels = "1871", pos = 2, srt = 90, cex = 0.6)
arrows(1893,0.3,1893,0.14, length = 0.1, col = "grey")
text(1893,0.3, labels = "1893", pos = 2, srt = 90, cex = 0.6)
arrows(1920,0.3,1920,0.26, length = 0.1, col = "grey")
text(1920,0.3, labels = "1920", pos = 2, srt = 90, cex = 0.6)
arrows(1955,0.3,1955,0.1, length = 0.1, col = "grey")
text(1955,0.3, labels = "1955", pos = 2, srt = 90, cex = 0.6)

u = 0
for( i in  words ) {

    #word = "been"
    word = i
    #cat(word, "\n")
    #b <- scan("stdin", character(), n=1)
    #Sys.sleep(1)
    # extracting the values for a given word, as a function of time
    current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))

    # assigning 0 to the non-existent values
    current.word[is.na(current.word)] = 0
    # applying a smoother
    current.word.smoother = lowess(current.word, f=1/5)$y
    if(sum(abs(current.word.smoother)) > threshold ) {
        lines(current.word.smoother ~ c(1835:1984), pch=20, col = rgb(0.8,0.2,0,0.2), lwd=3 )
        u = u + 1
    }
}

#lines(results.mean/300 ~ breakpoint.dates, col=rgb(0,0,1,0.6), lwd=5)
```

function words:

```{r funciton_words, echo=FALSE, message=FALSE, fig.width=8, fig.height=6, dpi=150}
word = "and"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
plot(current.word ~ c(1835:1984), ylim=c(-.3,.3), type = "n", xlab = "year", ylab = "discrimination strength")

arrows(1871,0.3,1871,0.2, length = 0.1, col = "grey")
text(1871,0.3, labels = "1871", pos = 2, srt = 90, cex = 0.6)
arrows(1918,0.3,1918,-0.03, length = 0.1, col = "grey")
text(1918,0.3, labels = "1918", pos = 2, srt = 90, cex = 0.6)
arrows(1929,0.3,1929,0.24, length = 0.1, col = "grey")
text(1929,0.3, labels = "1929", pos = 2, srt = 90, cex = 0.6)
arrows(1955,0.3,1955,0.1, length = 0.1, col = "grey")
text(1955,0.3, labels = "1955", pos = 2, srt = 90, cex = 0.6)

abline(h = 0, lty=2, col = "grey")
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.green, lwd=3 )

word = "that"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.violet, lwd=3 )

word = "is"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.red, lwd=3 )

word = "been"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.teal, lwd=3 )

word = "not"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.orange, lwd=3 )

word = "the"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.blue, lwd=3 )

word = "'s"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.grey, lwd=3 )

legend("bottomleft", legend = c("and", "that", "is", "been", "not", "the", "'s"),
    col = c(my.green, my.violet, my.red, my.teal, my.orange, my.blue, my.grey), 
    lwd = 3, bty = "n")

```

"social" words (personal pronouns):


```{r pers_pronouns, echo=FALSE, message=FALSE, fig.width=8, fig.height=6, dpi=150}
word = "you"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
plot(current.word ~ c(1835:1984), ylim=c(-.3,.3), type = "n", xlab = "year", ylab = "discrimination strength")

arrows(1871,0.3,1871,0.01, length = 0.1, col = "grey")
text(1871,0.3, labels = "1871", pos = 2, srt = 90, cex = 0.6)
arrows(1925,0.3,1925,0.01, length = 0.1, col = "grey")
text(1925,0.3, labels = "1925", pos = 2, srt = 90, cex = 0.6)
arrows(1959,0.3,1959,0.05, length = 0.1, col = "grey")
text(1959,0.3, labels = "1959", pos = 2, srt = 90, cex = 0.6)

abline(h = 0, lty=2, col = "grey")
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.blue, lwd=3 )

word = "my"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.green, lwd=3 )

word = "i"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.red, lwd=3 )

word = "your"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.teal, lwd=3 )

word = "we"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.grey, lwd=3 )

word = "our"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.violet, lwd=3 )

word = "n't"
# extracting the values for a given word, as a function of time
current.word = as.numeric(sapply(current.year.results, function(x) x[x[,2] == word, 4]))
current.word[is.na(current.word)] = 0
current.word.smoother = lowess(current.word, f=1/5)$y
lines(current.word.smoother ~ c(1835:1984), pch=20, col = my.orange, lwd=3 )

legend("bottomleft", legend = c("you", "my", "I", "your", "we", "our", "n't"),
    col = c(my.blue, my.green, my.red, my.teal, my.grey, my.violet, my.orange), 
    lwd = 3, bty = "n")

```




## Conclusions

in this paper we used a set of tailored stylometric methods to assess the question of language change over time. Our chosen techniques proved to be useful indeed; the further research will focus on tracing the very linguistic features that were responsible for the observed change. However, an important question has to be asked here: is it a change of Saussurean langue what we track with our approach, or rather the change of parole. Obviously, if texts written earlier can be separated from texts written more recently, they must share some features common for a given stage of language development. However, it is not clear if an observed change is due to, say, literary taste of the epoch or, if we face an actual change in the system here. Theoretically, the former and the latter are possible, as well as both answers together. It is also very likely that the change takes place in between: in the norm in the sense proposed by Coseriu (1958). Still there are no means to answer this question with any stylometric method, what for a linguist might be seen as a drawback. However, the proposed method informs the linguist about the fact of change, which takes place not only in lexis but also in syntax; about the speed of change and, above all, about the points where this speed accelerates.

## Acknowledgements

This research is part of project UMO-2013/11/B/HS2/02795, supported by Poland’s National Science Centre.

## References
